.\" Automatically generated by Pandoc 3.6.4
.\"
.TH "man 1" "October 2025" "5.2" "ear manpage"
.SH EAR overview
EAR is a system software for energy management, accounting, and
optimization for supercomputers.
This document provides a quick overview of how to use its services at
the user level.
The complete documentation can be found at the \c
.UR https://gitlab.bsc.es/ear_team/ear/-/wikis/home
official EAR wiki
.UE \c
\&.
You can also find useful \c
.UR https://gitlab.bsc.es/ear_team/ear/-/wikis/Tutorials
tutorials
.UE \c
\ there.
.PP
Below there is a list of specific details about the EAR software used in
this guide:
.IP \[bu] 2
Version: 5.2 > \f[B]Note\f[R] This user guide contains examples of job
submissions that may not fit exactly the submission instructions
specified by your cluster documentation.
Please refer to it to adapt batch job scripts shown here.
.SH Job submission
EAR fully supports two job schedulers: Slurm and PBS.
With the EAR Slurm SPANK plug\-in, running an application with EAR is as
easy as submitting a job with either \f[CR]srun\f[R], \f[CR]sbatch\f[R],
or \f[CR]mpirun\f[R].
With EAR hook for PBS, running an application with EAR is also as easy
as submitting the job with \f[CR]qsub\f[R].
The EAR runtime Library (EARL) is automatically loaded with some
applications when EAR is enabled by default.
.PP
Therefore, application performance metrics and power consumption can be
gathered online without code modification.
Moreover, you can enable energy optimization policies for your
workloads.
.SS Automatically fully supported use cases
.SS MPI applications (Including MPI + OpenMP/CUDA/MKL)
.IP \[bu] 2
Using \f[B]sbatch\f[R] + \f[B]srun\f[R]: The job submission with EAR is
totally automatic.
.IP \[bu] 2
Using \f[B]qsub\f[R] + \f[B]mpirun\f[R]: The job submission with EAR is
totally automatic.
.PP
Some EAR options can be requested at submission time:
.PP
For Slurm:
.IP
.EX
srun \-\-help \f[B]|\f[R] grep ear
.EE
.PP
For PBS, environmental variables are used via \f[B]#PBS \-v
\[lq]EAR_VAR=value\[rq]\f[R] or \f[B]qsub \-v
\[lq]EAR_VAR=value\[rq]\f[R].
To see the full list of available options type:
.IP
.EX
module load ear
ear\-hook\-help
.EE
.PP
If multiple steps are submitted in the same job, different flags or
variables can be used.
.PP
The following example executes two steps.
The first step uses default flags and the second one asks ear to report
ear metrics in a set of CSV files.
\f[CR]ear_metrics/app_metrics\f[R] is used as the root of filenames
generated.
.PP
For Slurm:
.IP
.EX
\f[I]#!/bin/bash\f[R]
\f[I]#SBATCH \-N 10\f[R]
\f[I]#SBATCH \-e test.%j.err \-o test.%j.out\f[R]
\f[I]#SBATCH \-\-tasks\-per\-node=24 \-\-cpus\-per\-task=1\f[R]
\f[I]#SBATCH \-\-ear=on\f[R]

module load mpi
\
mkdir ear_metrics
\f[I]# run application with ear\[aq]s default flags.\f[R]
srun \-n $SLURM_NTASKS application

\f[I]# run application and store ear metrics in ear_metrics/app_metrics.*.csv\f[R]
srun \-n $SLURM_NTASKS \-\-ear\-user\-db=ear_metrics/app_metrics application
.EE
.PP
For PBS:
.IP
.EX
\f[I]#!/bin/bash\f[R]
\f[I]#PBS \-l select=10:ppn=24\f[R]
\f[I]#PBS \-o test.out\f[R]
\f[I]#PBS \-e test.err\f[R]
\f[I]#PBS \-v \[dq]EAR=ON,EAR_USER_DB=ear_metrics/app_metrics\[dq]\f[R]

module load mpi
\
mkdir ear_metrics

\f[I]# run application and store ear metrics in ear_metrics/app_metrics.*.csv\f[R]
mpirun \-np 240 application

export EAR_USER_DB_PATHNAME=ear_metrics/app_metrics

mpirun \-np 240 application
.EE
.PP
Since in PBS there is no equivalent of \f[CR]srun\f[R], here we directly use
the \f[CR]mpirun\f[R] command.
.SS MPI versions
Using Intel \f[B]mpirun\f[R]: When running EAR with \f[CR]mpirun\f[R]
rather than \f[CR]srun\f[R], you must specify the utilisation of
\f[CR]slurm\f[R] as the bootstrap server.
.PP
Since \c
.UR https://www.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-linux/top/environment-variable-reference/hydra-environment-variables.html
Intel MPI\-2019
.UE \c
\ version there are two environment variables for bootstrap server
specification and arguments.
.IP \[bu] 2
\f[CR]I_MPI_HYDRA_BOOTSTRAP\f[R] sets the bootstrap server.
It must be set to \f[I]slurm\f[R] or \f[I]pbsdsh\f[R].
.IP \[bu] 2
\f[CR]I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS\f[R] sets additional
arguments for the bootstrap server.
.PP
The previous example batch script can be written (for an Intel MPI
application) as:
.IP
.EX
module load impi

export I_MPI_HYDRA_BOOTSTRAP=slurm
export I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS=\[dq]\-\-ear\-user\-db=ear_metrics/app_metrics\[dq]
mpiexec.hydra \-n 64 application
.EE
.IP \[bu] 2
Using OpenMPI\[cq]s \f[B]mpirun\f[R] with Slurm: It is recommended to
use \f[CR]srun\f[R] for OpenMPI applications.
If OpenMPI\[cq]s \f[CR]mpirun\f[R] is used instead, EAR will report just
job accounting metrics (DC Node Power and execution time of the job).
If you want to enable the EARL monitoring and optimization features, you
must use EAR\[cq]s \f[CR]erun\f[R] command before running your
application binary.
In this case, you must \f[B]load the ear module\f[R] installed in the
system.
.PP
In the case of PBS with Intel MPI:
.IP
.EX
\f[I]#PBS \-v \[dq]EAR_USER_DB=ear_metrics/app_metrics\[dq]\f[R]

module load impi

export I_MPI_HYDRA_BOOTSTRAP=pbsdsh
export I_MPI_HYDRA_ENV=all
mpiexec.hydra \-n 64 application
.EE
.IP \[bu] 2
In general, for any MPI version, the default behavior is to use
\f[B]mpirun\f[R] compiled with support to PBS.
.SS erun
\f[CR]erun\f[R] can be used to support any other use cases.
It accepts the same flags as \f[CR]sbatch\f[R]/\f[CR]srun\f[R] commands.
In addition, the \f[CR]\-\-program\f[R] flag is used to specify the
application you want to run.
See the following example:
.IP
.EX
\f[I]# PBS or Slurm requirements\f[R]

module load GROMACS/2024.2\-foss\-2023b
module load ear

mpirun \ erun \-\-ear\-verbose=1 \-\-program=\[dq]gmx_mpi mdrun \-ntomp 8 \-nb gpu \-pme gpu \-npme 1 \-update gpu \-bonded gpu \-nsteps 100000 \-resetstep 90000 \-noconfout \-dlb no \-nstlist 300 \-pin on \-v \-gpu_id 0123\[dq]
.EE
.SS Non\-MPI applications (CUDA, OpenMP, MKL and Python)
To enable EAR monitoring and optimization features for non\-MPI
applications, Slurm requires to run the application with the
\f[CR]srun\f[R] command.
For PBS it is not.
For \f[I]CUDA\f[R], \f[I]OpenMP\f[R], and \f[I]MKL\f[R] applications,
the binary must have been linked with dynamic symbols (e.g.,
\f[CR]\-\-cudart=shared\f[R]).
Below there is an example enabling EAR with an OpenMP application.
.PP
Slurm
.IP
.EX
\f[I]#!/bin/bash\f[R]

\f[I]#SBATCH \-N 1 \-n 1 \-\-cpus\-per\-task=64\f[R]
\f[I]#SBATCH \-\-ear=on \-\-ear\-verbose=1\f[R]

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

srun \-n $SLURM_NTASKS \-c $OMP_NUM_THREADS ./bt.D.x
.EE
.PP
PBS
.IP
.EX
\f[I]#!/bin/bash\f[R]

\f[I]#PBS select=1:cpus=64\f[R]
\f[I]#PBS \-v \[dq]EAR=on,EAR_VERBOSE=1\f[R]

export OMP_NUM_THREADS=64

mpirun \-np 1 ./bt.D.x
.EE
.PP
An example of running a Python application:
.PP
Slurm:
.IP
.EX
\f[I]#!/bin/bash\f[R]

\f[I]#SBATCH \-N 1 \-n 1 \-\-cpus\-per\-task=64\f[R]
\f[I]#SBATCH \-\-ear=on \-\-ear\-verbose=1\f[R]

srun \-n $SLURM_NTASKS \-c $SLURM_CPUS_PER_TASK python script.py
.EE
.PP
PBS:
.IP
.EX
\f[I]#!/bin/bash\f[R]

\f[I]#PBS select=1:cpus=64\f[R]
\f[I]#PBS \-v \[dq]EAR=on,EAR_VERBOSE=1\f[R]

python script.py
.EE
.SS Other use cases supported
.SS Python MPI applications
EAR can\[cq]t detect MPI symbols when Python is used, so an environment
variable is needed to specify which MPI implementation is being used.
.PP
Slurm:
.IP
.EX
module load ompi

export EAR_LOAD_MPI_VERSION=\[dq]open mpi\[dq]

srun \-n 64 \-\-ear\-user\-db=ear_metrics/app_metrics python script.py
.EE
.PP
PBS:
.IP
.EX
\f[I]#PBS \-v \[dq]EAR_USER_DB=ear_metrics/app_metrics\[dq]\f[R]
module load ompi

export EAR_LOAD_MPI_VERSION=\[dq]open mpi\[dq]

mpirun \-np 64 python script.py
.EE
.PP
For applications that use IntelMPI, the value must be \f[I]intel\f[R].
.SS Other application types or frameworks
For other programming models or sequential apps not supported by
default, EAR can be loaded by setting the
\f[CR]EAR_LOADER_APPLICATION\f[R] environment variable:
.PP
Slurm:
.IP
.EX
export EAR_LOADER_APPLICATION=/full/path/to/my_app

srun \-\-ear\-user\-db=ear_metrics/app_metrics my_app
.EE
.PP
PBS:
.IP
.EX
\f[I]#PBS \-v \[dq]EAR_USER_DB=ear_metrics/app_metrics\[dq]\f[R]

export EAR_LOADER_APPLICATION=/full/path/to/my_app

my_app
.EE
.SH Job accounting (eacct)
The \f[CR]eacct\f[R] command shows accounting information stored in the
EAR DB for jobs (and step) IDs.
You must first load the \f[B]ear\f[R] module.
Here the most useful command flags are listed:
.IP \[bu] 2
\f[CR]\-j <job_id>[.step_id]\f[R]: Specify the job (and optionally, the
step) you want to retrieve information.
.IP \[bu] 2
\f[CR]\-a <job_name>\f[R]: Specify the application name that will be
retrieved.
.IP \[bu] 2
\f[CR]\-c <filename>\f[R]: Store the output in csv format in <filename>.
Fields are separated by \f[CR];\f[R].
.IP \[bu] 2
\f[CR]\-l\f[R]: Specify you want job data for each of the used
computation nodes.
.IP \[bu] 2
\f[CR]\-r\f[R]: Request loop signatures instead of global application
metrics.
\f[B]EAR loop reporting must be enabled through
\f[CB]EARL_REPORT_LOOPS\f[B] environment variable if not automatic loop
reporting is enabled in your system.\f[R] Just set it to a non\-zero
value.
.IP \[bu] 2
\f[CR]\-s <YYYY\-MM\-DD>\f[R]: Specify the minimum start time of the
jobs that will be retrieved.
.IP \[bu] 2
\f[CR]\-e <YYYY\-MM\-DD>\f[R]: Specify the maximum end time of the jobs
that will be retrieved.
.SS Examples
The basic usage of eacct retrieves the last 20 applications (by default)
of the user executing it.
The default behaviour shows data from each job\-step, aggregating the
values from each node in said job\-step.
If using Slurm as a job manager, a sb (sbatch) job\-step is created with
the data from the entire execution.
A specific job may be specified with \f[CR]\-j\f[R] option:
.IP \[bu] 2
[user\[at]host EAR]$ eacct \[en]> Shows last 20 jobs (maximum) executed
by the user.
.IP \[bu] 2
[user\[at]host EAR]$ eacct \-j 175966 \[en]> Shows data for jobid =
175966.
Metrics are averaged per job.stepid.
.IP \[bu] 2
[user\[at]host EAR]$ eacct \-j 175966.0 \[en]> Shows data for jobid =
175966 stepid=0.
Metrics are averaged per job.stepid.
.IP \[bu] 2
[user\[at]host EAR]$ eacct \-j 175966,175967,175968 \[en]> Shows data
for jobid = 175966, 175967, 175968 Metrics are averaged per job.stepid.
.PP
Eacct shows a pre\-selected set of columns.
Some flags slightly modify the set of columns reported:
.IP \[bu] 2
JOB\-STEP: JobID and Step ID.
sb is shown for the sbatch.
.IP \[bu] 2
USER: Username who executed the job.
.IP \[bu] 2
APP=APPLICATION: Job\[cq]s name or executable name if job name is not
provided.
.IP \[bu] 2
POLICY: Energy optimization policy name (MO = Monitoring).
.IP \[bu] 2
NODES: Number of nodes that ran the job.
.IP \[bu] 2
AVG/DEF/IMC(GHz): Average CPU frequency, default frequency, and average
uncore frequency.
Includes all the nodes for the step.
In KHz.
.IP \[bu] 2
TIME(s): Step execution time, in seconds.
.IP \[bu] 2
POWER: Average node power including all the nodes, in Watts.
.IP \[bu] 2
GBS: CPU Main memory bandwidth (GB/second).
Hint for CPU/Memory bound classification.
.IP \[bu] 2
CPI: CPU Cycles per Instruction.
Hint for CPU/Memory bound classification.
.IP \[bu] 2
ENERGY(J): Accumulated node energy.
Includes all the nodes.
In Joules.
.IP \[bu] 2
GFLOPS/WATT: CPU GFlops per Watt.
Hint for energy efficiency.
.IP \[bu] 2
IO(MBs): IO (read and write) Mega Bytes per second.
.IP \[bu] 2
MPI%: Percentage of MPI time over the total execution time.
It\[cq]s the average including all the processes and nodes.
.IP \[bu] 2
GPU metrics \  \  \- G\-POW (T/U): Average GPU power.
Accumulated per node and average of all the nodes.
\  \  \  \  \- T= Total (GPU power consumed even if the process is not
using them).
\  \  \  \  \- U = GPUs used by the job.
\  \  \- G\-FREQ: Average GPU frequency.
Per node and average of all the nodes.
\  \  \- G\-UTIL(G/MEM): GPU utilization and GPU memory utilization.
.PP
The following example shows how to submit a job with EAR monitoring
enabled.
It also shows how to enable loop signatures reporting and finally how to
request the data.
.IP
.EX
\f[I]#!/bin/bash\f[R]
\f[I]#SBATCH \-J test\f[R]
\f[I]#SBATCH \-N 1\f[R]
\f[I]#SBATCH \-\-ntasks=112\f[R]
\f[I]#SBATCH \-\-cpus\-per\-task=1\f[R]

\f[I]#SBATCH \-\-ear=on\f[R]
\f[I]#SBATCH \-\-ear\-user\-db=metrics\f[R]

module purge
module load impi

export EARL_REPORT_LOOPS=1
srun ./bt\-mz.D.impi
.EE
.PP
With PBS:
.IP
.EX
\f[I]#PBS \-l select=1:ppn=112\f[R]
\f[I]#PBS \-o test.out\f[R]
\f[I]#PBS \-e test.err\f[R]
\f[I]#PBS \-v \[dq]EAR=ON,EAR_USER_DB=metrics\[dq]\f[R]

module purge
module load impi

export EARL_REPORT_LOOPS=1
mpirun ./bt\-mz.D.impi
.EE
.PP
Using \f[I]eacct\f[R] to retrieve loop signatures:
.IP
.EX
[user\[at]host bin]$ module load ear
[user\[at]host bin]$ eacct \-j 3180887 \-r
 JOB\-STEP NODE ID \  DATE \  \  POWER(W) GBS/TPI CPI \  GFLOPS/W TIME(s) AVG_F/F \ IMC_F IO(MBS) MPI% G\-POWER(T/U) \ G\-FREQ G\-UTIL(G/MEM)
3180887\-0 \  gs02r3b66 09:08:12 825.6 \  \ 156/17 \ 0.277 0.619 \  \ 1.013 \  2.52/2.0 1.81 \ 0.0 \  \  4.2 \ 0.0 \  / \  0.0 0.00 \  0%/0% \  \  \  \
3180887\-0 \  gs02r3b66 09:08:24 969.7 \  \ 157/17 \ 0.277 0.527 \  \ 1.240 \  2.51/2.0 1.81 \ 0.0 \  \  3.6 \ 0.0 \  / \  0.0 0.00 \  0%/0% \  \  \  \
3180887\-0 \  gs02r3b66 09:08:47 906.7 \  \ 157/17 \ 0.277 0.563 \  \ 1.127 \  2.51/2.0 1.81 \ 0.0 \  \  3.8 \ 0.0 \  / \  0.0 0.00 \  0%/0% \  \  \  \
3180887\-0 \  gs02r3b66 09:09:09 909.1 \  \ 157/17 \ 0.277 0.561 \  \ 1.126 \  2.51/2.0 1.81 \ 0.0 \  \  3.7 \ 0.0 \  / \  0.0 0.00 \  0%/0%
.EE
.PP
Using \f[I]eacct\f[R] to retrieve job signature:
.IP
.EX
[user\[at]host bin]$ eacct \-j 3180887
 JOB\-STEP USER \  \  \ APPLICATION POLICY NODES AVG/DEF/IMC(GHz) TIME(s) POWER(W) GBS \  \ CPI \ ENERGY(J) GFLOPS/W IO(MBs) MPI% G\-POW (T/U) G\-FREQ G\-UTIL(G/MEM)
3180887\-sb \  user \  \  \  \  \  \ test \  \  \  \ NP \  \  1 \  \  2.61/2.00/\-\-\- \  \ 120.00 \ 874.49 \  \-\-\- \  \ \-\-\- \ 104939 \  \ \-\-\- \  \  \ \-\-\- \  \  \-\-\- \ \-\-\- \  \  \  \  \-\-\- \  \ \-\-\- \  \  \  \  \
3180887\-0 \  \ user \  \  \  \ test \  \  \  \ MO \  \  1 \  \  2.52/2.00/1.81 \  97.72 \  913.51 \  157.12 0.28 89268 \  \  0.5578 \  0.0 \  \  3.7 \ 0.00/\-\-\- \  \ \-\-\- \  \ \-\-\-
.EE
